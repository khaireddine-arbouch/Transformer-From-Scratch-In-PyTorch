# Transformer-From-Scratch-In-PyTorch

This repository provides an implementation of the Transformer model from scratch using PyTorch. It serves as an educational resource to understand the inner workings of Transformers, a powerful model architecture that has revolutionized natural language processing (NLP) and other fields.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)

## Introduction

The Transformer model, introduced in the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762), has become the backbone of many state-of-the-art models in NLP, such as BERT, GPT, and T5. This repository aims to provide a clear and concise implementation of the Transformer architecture to help enthusiasts and researchers understand how it works under the hood.

## Features

- **Complete Transformer Architecture**: Includes the implementation of multi-head self-attention, position-wise feed-forward networks, positional encoding, and more.
- **Educational Focus**: Written with clarity in mind, with comments and explanations throughout the code to facilitate learning.
- **Customizable**: Easily modify and experiment with different configurations of the Transformer model.
- **Dependencies**: Built using PyTorch, making it easy to integrate with other PyTorch projects.

## Installation

To use this code, clone the repository and install the required dependencies:

```bash
git clone https://github.com/khaireddine-arbouch/Transformer-From-Scratch-In-PyTorch.git
cd Transformer-From-Scratch-In-PyTorch
pip install -r requirements.txt
```

- This project is inspired by the original Transformer paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762).
